{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update PacBio tables\n",
    "\n",
    "This notebook updates the 'sample' table of available PacBio flowcells, 'sample_set' table (one or more flowcells per participant), and 'participant' table (list of unique individuals discovered).\n",
    "\n",
    "To auto-populate these tables, this notebook scans files in the gs://broad-gp-pacbio bucket and extracts relevant metadata from the \\*.subreadset.xml files.\n",
    "\n",
    "If changes were made to the 'sample' table in Terra, we take care not to overwrite those changes. If one wishes to restore those entries to their original values, the rows should first be deleted from the Terra table.\n",
    "\n",
    "All other tables are auto-generated based on the 'sample' table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install and import some packages that we're going to need.  Set up some helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --use-feature=2020-resolver --upgrade pip pandas_gbq google-cloud-storage google-cloud-bigquery fastnumbers xmltodict > /dev/null 2>/dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import hashlib\n",
    "\n",
    "import pandas as pd\n",
    "import firecloud.api as fapi\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "from google.api_core.exceptions import NotFound\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import xmltodict\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0,
     31,
     44,
     68
    ]
   },
   "outputs": [],
   "source": [
    "def traverse_xml(key, xml):\n",
    "    tables = []\n",
    "    table = {}\n",
    "\n",
    "    for k in xml:\n",
    "        if 'xmlns' in k or 'xsi' in k:\n",
    "            continue\n",
    "\n",
    "        v = xml[k]\n",
    "\n",
    "        k = re.sub('^@|^#|^pbds:|^pbbase:', '', k)\n",
    "\n",
    "        l = []\n",
    "        if isinstance(v, str) or isinstance(v, dict):\n",
    "            l = [v]\n",
    "        elif isinstance(v, list):\n",
    "            l = v\n",
    "\n",
    "        for va in l:\n",
    "            if isinstance(va, str):\n",
    "                table[k] = v\n",
    "            if isinstance(va, dict):\n",
    "                f = traverse_xml(k, va)\n",
    "                tables.extend(f)\n",
    "\n",
    "    if len(table) > 0:\n",
    "        tables.append({key: table})\n",
    "\n",
    "    return tables\n",
    "\n",
    "\n",
    "def combine(tables):\n",
    "    combined_tables = {}\n",
    "\n",
    "    for table in tables:\n",
    "        for k in table:\n",
    "            if k not in combined_tables:\n",
    "                combined_tables[k] = []\n",
    "\n",
    "            combined_tables[k].append(table[k])\n",
    "\n",
    "    return combined_tables\n",
    "\n",
    "\n",
    "def load_xmls(gcs_buckets):\n",
    "    storage_client = storage.Client()\n",
    "    schemas = OrderedDict()\n",
    "\n",
    "    ts = []\n",
    "    for gcs_bucket in gcs_buckets:\n",
    "        blobs = storage_client.list_blobs(re.sub(\"^gs://\", \"\", gcs_bucket))\n",
    "\n",
    "        for blob in blobs:\n",
    "            if 'subreadset.xml' in blob.name:\n",
    "                xml = blob.download_as_string()\n",
    "                doc = xmltodict.parse(xml)\n",
    "\n",
    "                t = combine(traverse_xml('root', doc))\n",
    "                t['Files'] = {\n",
    "                    'subreadset.xml': gcs_bucket + \"/\" + blob.name,\n",
    "                    'subreads.bam': gcs_bucket + \"/\" + re.sub(\"et.xml\", \".bam\", blob.name),\n",
    "                    'subreads.bam.pbi': gcs_bucket + \"/\" + re.sub(\"et.xml\", \".bam.pbi\", blob.name)\n",
    "                }\n",
    "                ts.append(t)\n",
    "\n",
    "    return ts\n",
    "\n",
    "\n",
    "def upload_sample_set(namespace, workspace, tbl):\n",
    "    # delete old sample set\n",
    "    ss_old = fapi.get_entities(namespace, workspace, f'sample_set').json()\n",
    "    sample_sets = list(map(lambda e: e['name'], ss_old))\n",
    "    f = [fapi.delete_sample_set(namespace, workspace, sample_set_index) for sample_set_index in sample_sets]\n",
    "\n",
    "    # upload new sample set\n",
    "    ss = tbl.filter(['participant'], axis=1).drop_duplicates()\n",
    "    ss.columns = [f'entity:sample_set_id']\n",
    "    \n",
    "    b = fapi.upload_entities(namespace, workspace, entity_data=ss.to_csv(index=False, sep=\"\\t\"), model='flexible')\n",
    "    if b.status_code == 200:\n",
    "        print(f'Uploaded {len(ss)} sample sets successfully.')\n",
    "    else:\n",
    "        print(b.json())\n",
    "    \n",
    "    # upload membership set\n",
    "    ms = tbl.filter(['participant', 'entity:sample_id'], axis=1).drop_duplicates()\n",
    "    ms.columns = [f'membership:sample_set_id', f'sample']\n",
    "    \n",
    "    c = fapi.upload_entities(namespace, workspace, entity_data=ms.to_csv(index=False, sep=\"\\t\"), model='flexible')\n",
    "    if c.status_code == 200:\n",
    "        print(f'Uploaded {len(ms)} sample set members successfully.')\n",
    "    else:\n",
    "        print(c.json())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment\n",
    "\n",
    "Set up our environment (Terra namespace, workspace, and the location of PacBio bucket(s))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "namespace = os.environ['GOOGLE_PROJECT']\n",
    "workspace = os.environ['WORKSPACE_NAME']\n",
    "default_bucket = os.environ['WORKSPACE_BUCKET']\n",
    "\n",
    "gcs_buckets_pb = ['gs://broad-gp-pacbio']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve existing sample table from Terra\n",
    "\n",
    "If it exists, retrieve the 'sample' table from this workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_old = fapi.get_entities(namespace, workspace, 'sample').json()\n",
    "\n",
    "if len(ent_old) > 0:\n",
    "    tbl_old = pd.DataFrame(list(map(lambda e: e['attributes'], ent_old)))\n",
    "    tbl_old[\"entity:sample_id\"] = list(map(lambda f: hashlib.md5(f.encode(\"utf-8\")).hexdigest(), tbl_old[\"subreads_bam\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine PacBio subreadset.xml files\n",
    "\n",
    "Navigate GCS directories and look for PacBio flowcells (indicated by the presence of the *.subreadset.xml files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = load_xmls(gcs_buckets_pb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl_header = [\"subreads_bam\", \"subreads_pbi\", \"movie_name\", \"well_name\", \"created_at\", \"original_participant_name\", \"participant\", \"insert_size\", \"is_ccs\", \"isoseq\"]\n",
    "tbl_rows = []\n",
    "\n",
    "for e in ts:\n",
    "    tbl_rows.append([\n",
    "        e['Files']['subreads.bam'],\n",
    "        e['Files']['subreads.bam.pbi'],\n",
    "        e['CollectionMetadata'][0]['Context'] if 'Context' in e['CollectionMetadata'][0] else \"UnknownFlowcell\",\n",
    "        e['WellSample'][0]['WellName'] if 'WellName' in e['WellSample'][0] else \"Z00\",\n",
    "        e['WellSample'][0]['CreatedAt'] if 'CreatedAt' in e['WellSample'][0] else \"0001-01-01T00:00:00\",\n",
    "        re.sub(\"[# ]\", \"\", e['WellSample'][0]['Name']) if 'Name' in e['WellSample'][0] else \"UnknownSample\",\n",
    "        re.sub(\"[# ]\", \"\", e['WellSample'][0]['Name']) if 'Name' in e['WellSample'][0] else \"UnknownSample\",        \n",
    "        e['WellSample'][0]['InsertSize'] if 'InsertSize' in e['WellSample'][0] else \"0\",\n",
    "        e['WellSample'][0]['IsCCS'] if 'IsCCS' in e['WellSample'][0] else \"unknown\",\n",
    "        e['WellSample'][0]['IsoSeq'] if 'IsoSeq' in e['WellSample'][0] else \"unknown\"\n",
    "    ])\n",
    "    \n",
    "tbl_new = pd.DataFrame(tbl_rows, columns=tbl_header)\n",
    "tbl_new[\"entity:sample_id\"] = list(map(lambda f: hashlib.md5(f.encode(\"utf-8\")).hexdigest(), tbl_new[\"subreads_bam\"]))\n",
    "tbl_new[\"entity:ccs_sample\"] = list(map(lambda f: hashlib.md5(f.encode(\"utf-8\")).hexdigest(), tbl_new[\"subreads_bam\"]))\n",
    "tbl_new[\"entity:clr_sample\"] = list(map(lambda f: hashlib.md5(f.encode(\"utf-8\")).hexdigest(), tbl_new[\"subreads_bam\"]))\n",
    "tbl_new[\"entity:isoseq_sample\"] = list(map(lambda f: hashlib.md5(f.encode(\"utf-8\")).hexdigest(), tbl_new[\"subreads_bam\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge old and new sample list\n",
    "\n",
    "If there are changes to the old sample list, make sure we retain them through subsequent table updates.  Do not overwrite old sample entries (metadata may have been manually modified)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entry for sample 024ec5c45d7666816532e305e7593239 has been modified.  Keeping changes.\n",
      "Entry for sample 042a00a8f37554ef21089182a3c1cd59 has been modified.  Keeping changes.\n",
      "Entry for sample 049cca833af23f5ef945aea6cd08a8dd has been modified.  Keeping changes.\n",
      "Entry for sample 06c182e56fb4ea3bc7f14a92dc7365c6 has been modified.  Keeping changes.\n",
      "Entry for sample 0cf9da3fa74c763ef43df5d05acf1a89 has been modified.  Keeping changes.\n",
      "Entry for sample 0d0851af900a7a158535d1b9c5b06407 has been modified.  Keeping changes.\n",
      "Entry for sample 10309dcce0a3f24a3b2ec72e35415efa has been modified.  Keeping changes.\n",
      "Entry for sample 1e5f86309ad59034a1be8c078fa4cdaf has been modified.  Keeping changes.\n",
      "Entry for sample 2197cf420d18b7e94eed141adc4d33f4 has been modified.  Keeping changes.\n",
      "Entry for sample 2a40039bc98a22cafd08c5ab21ae4f46 has been modified.  Keeping changes.\n",
      "Entry for sample 2e3788ff5a00cb8519cdfb2f4a87dba8 has been modified.  Keeping changes.\n",
      "Entry for sample 305e169eef508922073cbd05e3dc9ed2 has been modified.  Keeping changes.\n",
      "Entry for sample 310144bacfdff367df3383b59b65997d has been modified.  Keeping changes.\n",
      "Entry for sample 34bafa6e6366d9423f944b9595e7bc5a has been modified.  Keeping changes.\n",
      "Entry for sample 3d1fe3f68bc33c7bff700416207b1878 has been modified.  Keeping changes.\n",
      "Entry for sample 4b87fd96c65eb3051ebe3411dcf16980 has been modified.  Keeping changes.\n",
      "Entry for sample 58ac6aca50ecb64ec770666468cc6936 has been modified.  Keeping changes.\n",
      "Entry for sample 662fc6f0081a502aed0e1d84bf534aee has been modified.  Keeping changes.\n",
      "Entry for sample 6922a699ac09da22330fe758300d0200 has been modified.  Keeping changes.\n",
      "Entry for sample 80f2f9d9a24bbd54588ec9e682adf675 has been modified.  Keeping changes.\n",
      "Entry for sample 893ab488f659815ec99cf1a7dff30be8 has been modified.  Keeping changes.\n",
      "Entry for sample 8ba10f6165d3c0120bfca96586cd806d has been modified.  Keeping changes.\n",
      "Entry for sample 93e0dc30b6f099872e515991a31e1213 has been modified.  Keeping changes.\n",
      "Entry for sample 96f6dc286068b4100696d7323fb7adaa has been modified.  Keeping changes.\n",
      "Entry for sample ad177decc81ebf8287438327f58127b5 has been modified.  Keeping changes.\n",
      "Entry for sample afd6431f4e73726df46283be7d56b028 has been modified.  Keeping changes.\n",
      "Entry for sample b003a404e12c6ca715e5f0520b1af74b has been modified.  Keeping changes.\n",
      "Entry for sample b08e1b7d52501f35eaed8d2800acdae9 has been modified.  Keeping changes.\n",
      "Entry for sample b42777702f2cc698fe8064579fdb9768 has been modified.  Keeping changes.\n",
      "Entry for sample b52bf0b1c8122b941ee8d28e82dd79dc has been modified.  Keeping changes.\n",
      "Entry for sample bcaeb1aef29c90876cd55a86050cd7a2 has been modified.  Keeping changes.\n",
      "Entry for sample c0d1231c6f6d210966ac1b10f5848db1 has been modified.  Keeping changes.\n",
      "Entry for sample c1219e4dcd79068329083050a5edba51 has been modified.  Keeping changes.\n",
      "Entry for sample c1528f19c9a3a3c1540adbdd081d0485 has been modified.  Keeping changes.\n",
      "Entry for sample d30813232765dcc6bfdbb1fdbd27dcc9 has been modified.  Keeping changes.\n",
      "Entry for sample d8a221a19a99748cadf79f5c50dd26dc has been modified.  Keeping changes.\n",
      "Entry for sample dd87bffb7d095079667bb6e26264bd8d has been modified.  Keeping changes.\n",
      "Entry for sample df165b04657f1cb4601fd6d67deb563c has been modified.  Keeping changes.\n",
      "Entry for sample e09488f46b14ecb780d2f6ef9d21624c has been modified.  Keeping changes.\n",
      "Entry for sample e18bfa122ac5a4831b669d302c9e94ab has been modified.  Keeping changes.\n",
      "Entry for sample e3fc023cb68bf765614937a08bf7328f has been modified.  Keeping changes.\n",
      "Entry for sample e78eaf3b028dbb8676bf610d0a80d4bd has been modified.  Keeping changes.\n",
      "Entry for sample eb324dc5689fab91479e60621b4d9e68 has been modified.  Keeping changes.\n",
      "Entry for sample f43b796523c54db3f1cf3c8a0e819627 has been modified.  Keeping changes.\n",
      "Entry for sample f943055577458614ff0f33aadd7ca807 has been modified.  Keeping changes.\n"
     ]
    }
   ],
   "source": [
    "if len(ent_old) > 0:\n",
    "    for sample_id in tbl_old.merge(tbl_new, how='outer', indicator=True).loc[lambda x : x['_merge']=='left_only']['entity:sample_id'].tolist():\n",
    "        print(f'Entry for sample {sample_id} has been modified.  Keeping changes.')\n",
    "        tbl_new = tbl_new[tbl_new['entity:sample_id'] != sample_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_tbl = pd.merge(tbl_old, tbl_new, how='outer') if len(ent_old) > 0 else tbl_new\n",
    "merged_tbl = merged_tbl[['entity:sample_id'] + tbl_header]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload new sample table to Terra\n",
    "\n",
    "Upload the merged 'sample' table to this workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded 111 rows successfully.\n"
     ]
    }
   ],
   "source": [
    "a = fapi.upload_entities(namespace, workspace, entity_data=merged_tbl.to_csv(index=False, sep=\"\\t\"), model='flexible')\n",
    "\n",
    "if a.status_code == 200:\n",
    "    print(f'Uploaded {len(merged_tbl)} rows successfully.')\n",
    "else:\n",
    "    print(a.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload sample_set table to Terra\n",
    "\n",
    "Create a 'sample_set' table that groups flowcells by participant name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded 73 sample sets successfully.\n",
      "Uploaded 111 sample set members successfully.\n"
     ]
    }
   ],
   "source": [
    "upload_sample_set(namespace, workspace, merged_tbl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "284px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
