{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update PacBio tables\n",
    "\n",
    "This notebook updates the 'sample' table of available PacBio flowcells, 'sample_set' table (one or more flowcells per participant), and 'participant' table (list of unique individuals discovered).\n",
    "\n",
    "To auto-populate these tables, this notebook scans files in the gs://broad-gp-pacbio bucket and extracts relevant metadata from the \\*.subreadset.xml files.\n",
    "\n",
    "If changes were made to the 'sample' table in Terra, we take care not to overwrite those changes. If one wishes to restore those entries to their original values, the rows should first be deleted from the Terra table.\n",
    "\n",
    "All other tables are auto-generated based on the 'sample' table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install and import some packages that we're going to need.  Set up some helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install --use-feature=2020-resolver --upgrade pip pandas_gbq google-cloud-storage google-cloud-bigquery fastnumbers xmltodict > /dev/null 2>/dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import hashlib\n",
    "\n",
    "import pandas as pd\n",
    "import firecloud.api as fapi\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "from google.api_core.exceptions import NotFound\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import xmltodict\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "code_folding": [
     0,
     31,
     44,
     93,
     102,
     129
    ]
   },
   "outputs": [],
   "source": [
    "def traverse_xml(key, xml):\n",
    "    tables = []\n",
    "    table = {}\n",
    "\n",
    "    for k in xml:\n",
    "        if 'xmlns' in k or 'xsi' in k:\n",
    "            continue\n",
    "\n",
    "        v = xml[k]\n",
    "\n",
    "        k = re.sub('^@|^#|^pbds:|^pbbase:|^pbmeta:|^pbsample:', '', k)\n",
    "\n",
    "        l = []\n",
    "        if isinstance(v, str) or isinstance(v, dict):\n",
    "            l = [v]\n",
    "        elif isinstance(v, list):\n",
    "            l = v\n",
    "\n",
    "        for va in l:\n",
    "            if isinstance(va, str):\n",
    "                table[k] = v\n",
    "            if isinstance(va, dict):\n",
    "                f = traverse_xml(k, va)\n",
    "                tables.extend(f)\n",
    "\n",
    "    if len(table) > 0:\n",
    "        tables.append({key: table})\n",
    "\n",
    "    return tables\n",
    "\n",
    "\n",
    "def combine(tables):\n",
    "    combined_tables = {}\n",
    "\n",
    "    for table in tables:\n",
    "        for k in table:\n",
    "            if k not in combined_tables:\n",
    "                combined_tables[k] = []\n",
    "\n",
    "            combined_tables[k].append(table[k])\n",
    "\n",
    "    return combined_tables\n",
    "\n",
    "\n",
    "def load_xmls(gcs_buckets):\n",
    "    storage_client = storage.Client()\n",
    "    schemas = OrderedDict()\n",
    "\n",
    "    ts = []\n",
    "    for gcs_bucket in gcs_buckets:\n",
    "        blobs = storage_client.list_blobs(re.sub(\"^gs://\", \"\", gcs_bucket))\n",
    "\n",
    "        for blob in blobs:\n",
    "            if 'subreadset.xml' in blob.name:\n",
    "                xml = blob.download_as_string()\n",
    "                doc = xmltodict.parse(xml)\n",
    "\n",
    "                t = combine(traverse_xml('root', doc))\n",
    "                t['Files'] = {\n",
    "                    'input_dir': os.path.dirname(gcs_bucket + \"/\" + blob.name),\n",
    "                    \n",
    "                    'subreadset.xml': gcs_bucket + \"/\" + blob.name,\n",
    "                    'subreads.bam': gcs_bucket + \"/\" + re.sub(\"et.xml\", \".bam\", blob.name),\n",
    "                    'subreads.bam.pbi': gcs_bucket + \"/\" + re.sub(\"et.xml\", \".bam.pbi\", blob.name),\n",
    "                    \n",
    "                    'consensusreadset.xml': \"\",\n",
    "                    'ccs_reports.txt': \"\",\n",
    "                    'reads.bam': \"\",\n",
    "                    'reads.bam.pbi': \"\"\n",
    "                }\n",
    "                ts.append(t)\n",
    "            elif 'consensusreadset.xml' in blob.name:\n",
    "                xml = blob.download_as_string()\n",
    "                doc = xmltodict.parse(xml)\n",
    "\n",
    "                t = combine(traverse_xml('root', doc))\n",
    "                t['Files'] = {\n",
    "                    'input_dir': os.path.dirname(gcs_bucket + \"/\" + blob.name),\n",
    "                    \n",
    "                    'subreadset.xml': \"\",\n",
    "                    'subreads.bam': \"\",\n",
    "                    'subreads.bam.pbi': \"\",\n",
    "\n",
    "                    'consensusreadset.xml': gcs_bucket + \"/\" + blob.name,\n",
    "                    'ccs_reports.txt': gcs_bucket + \"/\" + re.sub(\".consensusreadset.xml\", \".ccs_reports.txt\", blob.name),\n",
    "                    'reads.bam': gcs_bucket + \"/\" + re.sub(\".consensusreadset.xml\", \".reads.bam\", blob.name),\n",
    "                    'reads.bam.pbi': gcs_bucket + \"/\" + re.sub(\".consensusreadset.xml\", \".reads.bam.pbi\", blob.name)\n",
    "                }\n",
    "                ts.append(t)\n",
    "\n",
    "    return ts\n",
    "\n",
    "\n",
    "def upload_samples(namespace, workspace, tbl):\n",
    "    a = fapi.upload_entities(namespace, workspace, entity_data=tbl.to_csv(index=False, sep=\"\\t\"), model='flexible')\n",
    "\n",
    "    if a.status_code == 200:\n",
    "        print(f'Uploaded {len(tbl)} rows successfully.')\n",
    "    else:\n",
    "        print(a.json())\n",
    "\n",
    "\n",
    "def upload_sample_set(namespace, workspace, tbl):\n",
    "    # delete old sample set\n",
    "    ss_old = fapi.get_entities(namespace, workspace, f'sample_set').json()\n",
    "    sample_sets = list(map(lambda e: e['name'], ss_old))\n",
    "    f = [fapi.delete_sample_set(namespace, workspace, sample_set_index) for sample_set_index in sample_sets]\n",
    "\n",
    "    # upload new sample set\n",
    "    ss = tbl.filter(['bio_sample'], axis=1).drop_duplicates()\n",
    "    ss.columns = [f'entity:sample_set_id']\n",
    "    \n",
    "    b = fapi.upload_entities(namespace, workspace, entity_data=ss.to_csv(index=False, sep=\"\\t\"), model='flexible')\n",
    "    if b.status_code == 200:\n",
    "        print(f'Uploaded {len(ss)} sample sets successfully.')\n",
    "    else:\n",
    "        print(b.json())\n",
    "    \n",
    "    # upload membership set\n",
    "    ms = tbl.filter(['bio_sample', 'entity:sample_id'], axis=1).drop_duplicates()\n",
    "    ms.columns = [f'membership:sample_set_id', f'sample']\n",
    "    \n",
    "    c = fapi.upload_entities(namespace, workspace, entity_data=ms.to_csv(index=False, sep=\"\\t\"), model='flexible')\n",
    "    if c.status_code == 200:\n",
    "        print(f'Uploaded {len(ms)} sample set members successfully.')\n",
    "    else:\n",
    "        print(c.json())\n",
    "\n",
    "\n",
    "def load_ccs_report(ccs_report_path):\n",
    "    d = {\n",
    "        'ZMWs input': \"\",\n",
    "        'ZMWs pass filters': \"\",\n",
    "        'ZMWs fail filters': \"\",\n",
    "        'ZMWs shortcut filters': \"\",\n",
    "        'ZMWs with tandem repeats': \"\",\n",
    "        'Below SNR threshold': \"\",\n",
    "        'Median length filter': \"\",\n",
    "        'Lacking full passes': \"\",\n",
    "        'Heteroduplex insertions': \"\",\n",
    "        'Coverage drops': \"\",\n",
    "        'Insufficient draft cov': \"\",\n",
    "        'Draft too different': \"\",\n",
    "        'Draft generation error': \"\",\n",
    "        'Draft above --max-length': \"\",\n",
    "        'Draft below --min-length': \"\",\n",
    "        'Reads failed polishing': \"\",\n",
    "        'Empty coverage windows': \"\",\n",
    "        'CCS did not converge': \"\",\n",
    "        'CCS below minimum RQ': \"\",\n",
    "        'Unknown error': \"\"\n",
    "    }\n",
    "    \n",
    "    if ccs_report_path != \"\":\n",
    "        storage_client = storage.Client()\n",
    "\n",
    "        ccs_report = re.sub(\"^gs://\", \"\", e['Files']['ccs_reports.txt']).split(\"/\")\n",
    "        blobs = storage_client.list_blobs(ccs_report[0], prefix=\"/\".join(ccs_report[1:]))\n",
    "\n",
    "        for blob in blobs:\n",
    "            blob.download_to_filename(\"ccs_report.txt\")\n",
    "\n",
    "            file = open(\"ccs_report.txt\", \"r\")\n",
    "\n",
    "            d = {}\n",
    "            for line in file:\n",
    "                if len(line) > 1 and 'Exclusive counts for ZMWs' not in line:\n",
    "                    a = line.rstrip().split(\":\")\n",
    "\n",
    "                    k = a[0].rstrip()\n",
    "                    v = float(re.sub(\" \", \"\", re.sub(\" \\(.*$\", \"\", a[1])))\n",
    "\n",
    "                    if k not in d:\n",
    "                        d[k] = 0.0;\n",
    "\n",
    "                    d[k] = d[k] + v\n",
    "\n",
    "            break\n",
    "            \n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment\n",
    "\n",
    "Set up our environment (Terra namespace, workspace, and the location of PacBio bucket(s))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "namespace = os.environ['GOOGLE_PROJECT']\n",
    "workspace = os.environ['WORKSPACE_NAME']\n",
    "default_bucket = os.environ['WORKSPACE_BUCKET']\n",
    "\n",
    "gcs_buckets_pb = ['gs://broad-gp-pacbio']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve existing sample table from Terra\n",
    "\n",
    "If it exists, retrieve the 'sample' table from this workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_old = fapi.get_entities(namespace, workspace, 'sample').json()\n",
    "\n",
    "if len(ent_old) > 0:\n",
    "    tbl_old = pd.DataFrame(list(map(lambda e: e['attributes'], ent_old)))\n",
    "    tbl_old[\"entity:sample_id\"] = list(map(lambda f: hashlib.md5(f.encode(\"utf-8\")).hexdigest(), tbl_old[\"subreads_bam\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine PacBio subreadset.xml files\n",
    "\n",
    "Navigate GCS directories and look for PacBio flowcells (indicated by the presence of the *.subreadset.xml files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = load_xmls(gcs_buckets_pb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl_header = [\"entity:sample_id\", \"instrument\", \"movie_name\", \"well_name\", \"created_at\", \"bio_sample\", \"well_sample\", \"insert_size\", \"is_ccs\", \"is_isoseq\", \"num_records\", \"total_length\", \"zmws_input\", \"zmws_pass\", \"zmws_fail\", \"zmws_shortcut_filters\", \"gcs_input_dir\", \"subreads_bam\", \"subreads_pbi\", \"ccs_bam\", \"ccs_pbi\"]\n",
    "tbl_rows = []\n",
    "\n",
    "for e in ts:\n",
    "    r = load_ccs_report(e['Files']['ccs_reports.txt'])\n",
    "    \n",
    "    tbl_rows.append([\n",
    "        e['CollectionMetadata'][0]['UniqueId'] if 'Context' in e['CollectionMetadata'][0] else \"\",\n",
    "\n",
    "        e['CollectionMetadata'][0]['InstrumentName'] if 'Context' in e['CollectionMetadata'][0] else \"UnknownInstrument\",\n",
    "        e['CollectionMetadata'][0]['Context'] if 'Context' in e['CollectionMetadata'][0] else \"UnknownFlowcell\",\n",
    "        e['WellSample'][0]['WellName'] if 'WellName' in e['WellSample'][0] else \"Z00\",\n",
    "        e['WellSample'][0]['CreatedAt'] if 'CreatedAt' in e['WellSample'][0] else \"0001-01-01T00:00:00\",\n",
    "        re.sub(\"[# ]\", \"\", e['BioSample'][0]['Name']) if 'BioSample' in e else \"UnknownBioSample\",\n",
    "        re.sub(\"[# ]\", \"\", e['WellSample'][0]['Name']) if 'Name' in e['WellSample'][0] else \"UnknownWellSample\",\n",
    "        e['WellSample'][0]['InsertSize'] if 'InsertSize' in e['WellSample'][0] else \"0\",\n",
    "        e['WellSample'][0]['IsCCS'] if 'IsCCS' in e['WellSample'][0] else \"unknown\",\n",
    "        e['WellSample'][0]['IsoSeq'] if 'IsoSeq' in e['WellSample'][0] else \"unknown\",\n",
    "        \n",
    "        e['DataSetMetadata'][0]['NumRecords'],\n",
    "        e['DataSetMetadata'][0]['TotalLength'],\n",
    "        \n",
    "        r['ZMWs input'],\n",
    "        r['ZMWs pass filters'],\n",
    "        r['ZMWs fail filters'],\n",
    "        r['ZMWs shortcut filters'],\n",
    "\n",
    "        e['Files']['input_dir'],\n",
    "        e['Files']['subreads.bam'],\n",
    "        e['Files']['subreads.bam.pbi'],\n",
    "        e['Files']['reads.bam'],\n",
    "        e['Files']['reads.bam.pbi'],\n",
    "    ])\n",
    "    \n",
    "tbl_new = pd.DataFrame(tbl_rows, columns=tbl_header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge old and new sample list\n",
    "\n",
    "If there are changes to the old sample list, make sure we retain them through subsequent table updates.  Do not overwrite old sample entries (metadata may have been manually modified)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(ent_old) > 0:\n",
    "    for sample_id in tbl_old.merge(tbl_new, how='outer', indicator=True).loc[lambda x : x['_merge']=='left_only']['entity:sample_id'].tolist():\n",
    "        print(f'Entry for sample {sample_id} has been modified.  Keeping changes.')\n",
    "        tbl_new = tbl_new[tbl_new['entity:sample_id'] != sample_id]\n",
    "        \n",
    "merged_tbl = pd.merge(tbl_old, tbl_new, how='outer') if len(ent_old) > 0 else tbl_new\n",
    "merged_tbl = merged_tbl.drop_duplicates(subset=['entity:sample_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload new sample table to Terra\n",
    "\n",
    "Upload the merged 'sample' table to this workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded 133 rows successfully.\n"
     ]
    }
   ],
   "source": [
    "upload_samples(namespace, workspace, merged_tbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload sample_set table to Terra\n",
    "\n",
    "Create a 'sample_set' table that groups flowcells by participant name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded 83 sample sets successfully.\n",
      "Uploaded 133 sample set members successfully.\n"
     ]
    }
   ],
   "source": [
    "upload_sample_set(namespace, workspace, merged_tbl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "284px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
