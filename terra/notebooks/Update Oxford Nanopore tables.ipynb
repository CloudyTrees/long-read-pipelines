{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update Oxford Nanopore tables\n",
    "\n",
    "This notebook updates the 'sample' table of available ONT flowcells, 'sample_set' table (one or more flowcells per participant), and 'participant' table (list of unique individuals discovered).\n",
    "\n",
    "To auto-populate these tables, this notebook scans files in the gs://broad-gp-oxfordnano and gs://broad-dsde-methods-long-reads-deepseq buckets and extracts relevant metadata from the final_summary.txt files.\n",
    "\n",
    "If changes were made to the 'sample' table in Terra, we take care not to overwrite those changes. If one wishes to restore those entries to their original values, the rows should first be deleted from the Terra table.\n",
    "\n",
    "All other tables are auto-generated based on the 'sample' table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install and import some packages that we're going to need.  Set up some helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --use-feature=2020-resolver --upgrade pip pandas_gbq google-cloud-storage google-cloud-bigquery fastnumbers xmltodict > /dev/null 2>/dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import hashlib\n",
    "\n",
    "import pandas as pd\n",
    "import firecloud.api as fapi\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "from google.api_core.exceptions import NotFound\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import xmltodict\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     31
    ]
   },
   "outputs": [],
   "source": [
    "def load_summaries(gcs_buckets):\n",
    "    storage_client = storage.Client()\n",
    "    schemas = OrderedDict()\n",
    "\n",
    "    ts = []\n",
    "    for gcs_bucket in gcs_buckets:\n",
    "        blobs = storage_client.list_blobs(re.sub(\"^gs://\", \"\", gcs_bucket))\n",
    "\n",
    "        for blob in blobs:\n",
    "            if 'final_summary' in blob.name:\n",
    "                doc = blob.download_as_string()\n",
    "                t = {}\n",
    "                \n",
    "                for line in doc.decode(\"utf-8\").split(\"\\n\"):\n",
    "                    if '=' in line:\n",
    "                        k,v = line.split('=')\n",
    "                    \n",
    "                        t[k] = v\n",
    "                        \n",
    "                t['Files'] = {\n",
    "                    'final_summary.txt': gcs_bucket + \"/\" + blob.name,\n",
    "                    'sequencing_summary.txt': 'missing'\n",
    "                }\n",
    "\n",
    "                bs = storage_client.list_blobs(re.sub(\"^gs://\", \"\", gcs_bucket), prefix=os.path.dirname(blob.name) + \"/\" + t['sequencing_summary_file'])\n",
    "                for b in bs:\n",
    "                    t['Files']['sequencing_summary.txt'] = gcs_bucket + \"/\" + b.name\n",
    "                    \n",
    "                if 'sequencing_summary.txt' not in t['Files']:\n",
    "                    pp = pprint.PrettyPrinter(indent=4)\n",
    "                    pp.pprint(t)\n",
    "\n",
    "                ts.append(t)\n",
    "\n",
    "    return ts\n",
    "\n",
    "\n",
    "def upload_sample_set(namespace, workspace, tbl):\n",
    "    # delete old sample set\n",
    "    ss_old = fapi.get_entities(namespace, workspace, f'sample_set').json()\n",
    "    sample_sets = list(map(lambda e: e['name'], ss_old))\n",
    "    f = [fapi.delete_sample_set(namespace, workspace, sample_set_index) for sample_set_index in sample_sets]\n",
    "\n",
    "    # upload new sample set\n",
    "    ss = tbl.filter(['participant'], axis=1).drop_duplicates()\n",
    "    ss.columns = [f'entity:sample_set_id']\n",
    "    \n",
    "    b = fapi.upload_entities(namespace, workspace, entity_data=ss.to_csv(index=False, sep=\"\\t\"), model='flexible')\n",
    "    if b.status_code == 200:\n",
    "        print(f'Uploaded {len(ss)} sample sets successfully.')\n",
    "    else:\n",
    "        print(b.json())\n",
    "    \n",
    "    # upload membership set\n",
    "    ms = tbl.filter(['participant', 'entity:sample_id'], axis=1).drop_duplicates()\n",
    "    ms.columns = [f'membership:sample_set_id', f'sample']\n",
    "    \n",
    "    c = fapi.upload_entities(namespace, workspace, entity_data=ms.to_csv(index=False, sep=\"\\t\"), model='flexible')\n",
    "    if c.status_code == 200:\n",
    "        print(f'Uploaded {len(ms)} sample set members successfully.')\n",
    "    else:\n",
    "        print(c.json())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment\n",
    "\n",
    "Set up our environment (Terra namespace, workspace, and the location of ONT bucket(s))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "namespace = os.environ['GOOGLE_PROJECT']\n",
    "workspace = os.environ['WORKSPACE_NAME']\n",
    "default_bucket = os.environ['WORKSPACE_BUCKET']\n",
    "\n",
    "gcs_buckets_ont = ['gs://broad-gp-oxfordnano', 'gs://broad-dsde-methods-long-reads-deepseq']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve existing sample table from Terra\n",
    "\n",
    "If it exists, retrieve the 'sample' table from this workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_old = fapi.get_entities(namespace, workspace, 'sample').json()\n",
    "\n",
    "if len(ent_old) > 0:\n",
    "    tbl_old = pd.DataFrame(list(map(lambda e: e['attributes'], ent_old)))\n",
    "    tbl_old[\"entity:sample_id\"] = list(map(lambda f: hashlib.md5(f.encode(\"utf-8\")).hexdigest(), tbl_old[\"final_summary_file\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine Oxford Nanopore final_summary.txt files\n",
    "\n",
    "Navigate GCS directories and look for ONT flowcells (indicated by the presence of the final_summary.txt files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = load_summaries(gcs_buckets_ont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl_header = [\"final_summary_file\", \"sequencing_summary_file\", \"protocol_group_id\", \"instrument\", \"position\", \"flow_cell_id\", \"original_participant_name\", \"participant\", \"basecalling_enabled\", \"started\", \"acquisition_stopped\", \"processing_stopped\", \"fast5_files_in_fallback\", \"fast5_files_in_final_dest\", \"fastq_files_in_fallback\", \"fastq_files_in_final_dest\"]\n",
    "tbl_rows = []\n",
    "\n",
    "for e in ts:\n",
    "    tbl_rows.append([\n",
    "        e[\"Files\"][\"final_summary.txt\"],\n",
    "        e[\"Files\"][\"sequencing_summary.txt\"],\n",
    "        e[\"protocol_group_id\"],\n",
    "        e[\"instrument\"],\n",
    "        e[\"position\"],\n",
    "        e[\"flow_cell_id\"],\n",
    "        e[\"sample_id\"],\n",
    "        e[\"sample_id\"],\n",
    "        e[\"basecalling_enabled\"],\n",
    "        e[\"started\"],\n",
    "        e[\"acquisition_stopped\"],\n",
    "        e[\"processing_stopped\"],\n",
    "        e[\"fast5_files_in_fallback\"],\n",
    "        e[\"fast5_files_in_final_dest\"],\n",
    "        e[\"fastq_files_in_fallback\"],\n",
    "        e[\"fastq_files_in_final_dest\"]\n",
    "    ])\n",
    "    \n",
    "tbl_new = pd.DataFrame(tbl_rows, columns=tbl_header)\n",
    "tbl_new[\"entity:sample_id\"] = list(map(lambda f: hashlib.md5(f.encode(\"utf-8\")).hexdigest(), tbl_new[\"final_summary_file\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge old and new sample list\n",
    "\n",
    "If there are changes to the old sample list, make sure we retain them through subsequent table updates.  Do not overwrite old sample entries (metadata may have been manually modified)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(ent_old) > 0:\n",
    "    for sample_id in tbl_old.merge(tbl_new, how='outer', indicator=True).loc[lambda x : x['_merge']=='left_only']['entity:sample_id'].tolist():\n",
    "        print(f'Entry for sample {sample_id} has been modified.  Keeping changes.')\n",
    "        tbl_new = tbl_new[tbl_new['entity:sample_id'] != sample_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_tbl = pd.merge(tbl_old, tbl_new, how='outer') if len(ent_old) > 0 else tbl_new\n",
    "merged_tbl = merged_tbl[['entity:sample_id'] + tbl_header]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload new sample table to Terra\n",
    "\n",
    "Upload the merged 'sample' table to this workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = fapi.upload_entities(namespace, workspace, entity_data=merged_tbl.to_csv(index=False, sep=\"\\t\"), model='flexible')\n",
    "\n",
    "if a.status_code == 200:\n",
    "    print(f'Uploaded {len(merged_tbl)} rows successfully.')\n",
    "else:\n",
    "    print(a.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload sample_set table to Terra\n",
    "\n",
    "Create a 'sample_set' table that groups flowcells by participant name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_sample_set(namespace, workspace, merged_tbl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "284px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
